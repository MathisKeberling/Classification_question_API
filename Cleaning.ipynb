{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d6b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautiful soup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Caracter replacement \n",
    "import re\n",
    "\n",
    "# Tokenizer \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import words, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484dbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/william/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/william/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/william/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/william/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/william/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b82499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition des stopwords, après observation des mots à l'aide d'un wordcloud on peut etendre cette liste \n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['like','want', 'would', 'also', 'however', 'something', 'example', 'one', 'see', 'could', 'trying', 'tried', 'thank', 'thanks'])\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0d335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_vf(doc,min_len_word = 3):\n",
    "    '''\n",
    "    \n",
    "    positionnal arguments : \n",
    "    -----------------------\n",
    "    doc : str : the docuemnt (aka a text in str format) to process\n",
    "    \n",
    "    opt args : \n",
    "    -----------------------\n",
    "    rejoin : bool if True return a string else return a list of tokens\n",
    "    lemm_or_stem : string : if lem do lemmantize else stemmentize\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : the minimum length of word to not exclude\n",
    "    \n",
    "    return :\n",
    "    -----------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    '''\n",
    "        \n",
    "    #lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # Supprimer le code, flags prend en compte egalement les sauts de ligne \n",
    "    doc = re.sub('<code>.*?</code>', '', doc, flags=re.DOTALL)\n",
    "    \n",
    "    # Supprimer les balises html de notre corps de question \n",
    "    doc = bs(doc, \"lxml\").text\n",
    "    \n",
    "    # Suppresion des retours à la ligne \n",
    "    to_clean = re.compile('\\n')\n",
    "    doc = re.sub(to_clean, ' ', doc)\n",
    "\n",
    "    \n",
    "    #Supprimer les url\n",
    "    doc = re.sub(r'http*\\S+', '', doc)\n",
    "    # Supprimer les espace inutiles \n",
    "    doc = re.sub('\\\\s+', ' ', doc)\n",
    "    # Supprimer les nombres\n",
    "    doc = re.sub(r'\\w*\\d+\\w*', '', doc)\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w+#?\\b')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    \n",
    "    # classic stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    \n",
    "    # no more len words\n",
    "    more_than_N = [w for w in cleaned_tokens_list if len(w)>= min_len_word]\n",
    "    \n",
    "    # lem\n",
    "    trans = WordNetLemmatizer()\n",
    "    trans_text = [trans.lemmatize(i) for i in more_than_N]\n",
    "    \n",
    "    return trans_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
